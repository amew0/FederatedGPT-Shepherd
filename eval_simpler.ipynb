{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "logg = lambda x: print(f\"------------------------ {x} ---------------------------\")\n",
    "from datetime import datetime\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------ 240628085810 ---------------------------\n",
      "------------------------ eval_simpler.py ---------------------------\n"
     ]
    }
   ],
   "source": [
    "ME = \"/dpc/kunf0097/l3-8b\"\n",
    "RUN_ID = datetime.now().strftime(\"%y%m%d%H%M%S\")\n",
    "logg(RUN_ID)\n",
    "logg('eval_simpler.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "name = \"amew0/l3-8b-medical-v240622193618\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    name, \n",
    "    cache_dir=f\"{ME}/tokenizer\", \n",
    "    padding_side=\"right\", \n",
    "    pad_token_id=0,\n",
    "    legacy=False\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01b64401f3bf4e4a98a18d96f1541995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize model\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "# Initialize model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    name, \n",
    "    cache_dir=f\"{ME}/model\",\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0},\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize prompt function\n",
    "cutoff_len = 128  # Maximum length to control CUDA OOM\n",
    "\n",
    "def tokenize(prompt, tokenizer, add_eos_token=True):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=cutoff_len,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    result[\"input_ids\"] = result[\"input_ids\"].flatten()\n",
    "    result[\"attention_mask\"] = result[\"attention_mask\"].flatten()\n",
    "\n",
    "    if add_eos_token and result[\"input_ids\"].shape[0] < cutoff_len:\n",
    "        result[\"input_ids\"][-1] = tokenizer.eos_token_id\n",
    "        result[\"attention_mask\"][-1] = 1\n",
    "\n",
    "    result[\"labels\"] = result[\"input_ids\"].clone()\n",
    "    return result\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    tokenized_full_prompt = tokenize(data_point[\"prompt\"], tokenizer=tokenizer).to('cpu')\n",
    "    return tokenized_full_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09d1cccc35ce41a285baf0124d73e432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_path = './data/1/medical.json'\n",
    "data = load_dataset(\"json\", data_files=data_path)\n",
    "eval_dataset = data[\"train\"].shuffle().map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_squad_predictions(pred):\n",
    "    labels = pred[\"labels\"].flatten()\n",
    "    logits = pred[\"logits\"].argmax(-1).flatten()\n",
    "\n",
    "    decoded_labels = tokenizer.decode(labels)\n",
    "    decoded_logits = tokenizer.decode(logits)\n",
    "    print(decoded_labels)\n",
    "    print(decoded_logits)\n",
    "\n",
    "    em = sum([1 if p == l else 0 for p, l in zip(labels, logits)]) / len(labels)\n",
    "    f1 = f1_score(labels, logits, average='macro')\n",
    "\n",
    "    return {'exact_match': em, 'f1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|> If you are a doctor, please answer the medical questions based on the patient's description.<|eot_id|><|start_header_id|>user<|end_header_id|> This is the question: Hello doctor..My name kamal I m 21 years / male.......l sufrred from a depression I m afraid when I m go to bed and couldn t sleep then everyday I thinking how I can sleep today then this illness is effect on my education then my health then cause no thing make me happy still nw I forget this feeling. When I have any work nextday I couldn t sleep and I m thinking what happen nextday then if I couldn t sleep How I\n",
      "галіsystem글상위 If you are a doctor, please answer the medical questions based on the patient's description.글상위글상위user� This is the question: I,, I wife islesh k am 26 years old ..IIatelyfering from a severe and m taking of I m alone out sleep I I t sleep for I I m about to m sleep and and I is is very my my study and I marks and my of confidence to me happy and now I m how illness and I I m a problem I day I can t do then thinking m thinking how I today I I I m t sleep I I can\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "from tqdm import tqdm\n",
    "for i, example in tqdm(enumerate(eval_dataset)):\n",
    "    with torch.no_grad():\n",
    "        example[\"input_ids\"] = torch.LongTensor(example[\"input_ids\"]).unsqueeze(0)\n",
    "        example[\"attention_mask\"] = torch.LongTensor(example[\"attention_mask\"]).unsqueeze(0)\n",
    "\n",
    "        outputs = model(input_ids=example[\"input_ids\"], \n",
    "                        attention_mask=example[\"attention_mask\"])\n",
    "    pred = {\n",
    "        \"labels\": example[\"input_ids\"],\n",
    "        \"logits\": outputs[\"logits\"],\n",
    "    }\n",
    "    \n",
    "    # Evaluate predictions\n",
    "    eval_results = evaluate_squad_predictions(pred)\n",
    "    results.append(eval_results)\n",
    "    if i == 0: break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from the hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4b06b2cbf034bea8c76bb67d34f38b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "name = \"amew0/l3-8b-medical-v240623023136\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    name, \n",
    "    cache_dir=f\"{ME}/tokenizer\", \n",
    "    # padding_side=\"right\", \n",
    "    pad_token_id=0,\n",
    "    # legacy=False\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    name,\n",
    "    cache_dir=f\"{ME}/model\", \n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000, 128006,   9125, 128007,    271,   2675,    527,    264,  55066,\n",
       "           6369,   6465,    889,   2744,  31680,    304,  55066,   6604,      0,\n",
       "         128009, 128006,    882, 128007,    271,  15546,    527,    499,     30,\n",
       "         128009, 128006,  78191, 128007,    271]], device='cuda:0')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = tokenizer(\"<|start_header_id|>system<|end_header_id|>\\n\\nYou are a pirate chatbot who always responds in pirate speak!<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWho are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    "    return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[128000, 128006,   9125, 128007,    271,   2675,    527,    264,  55066,\n",
       "           6369,   6465,    889,   2744,  31680,    304,  55066,   6604,      0,\n",
       "         128009, 128006,    882, 128007,    271,  15546,    527,    499,     30,\n",
       "         128009, 128006,  78191, 128007,    271]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(\n",
    "    input_ids=tokenized[\"input_ids\"],\n",
    "    attention_mask = tokenized[\"attention_mask\"],\n",
    "    max_new_tokens=64,\n",
    "    eos_token_id=[\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ],\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000, 128006,   9125, 128007,    271,   2675,    527,    264,  55066,\n",
       "           6369,   6465,    889,   2744,  31680,    304,  55066,   6604,      0,\n",
       "         128009, 128006,    882, 128007,    271,  15546,    527,    499,     30,\n",
       "         128009, 128006,  78191, 128007,    271,   9014,    637,     11,    757,\n",
       "          82651,      0,   2206,    836,    387,  13149,  19150,     11,    279,\n",
       "          70355,    713,    297,      6,    279,   8254,  52840,      0,   2206,\n",
       "            264,  55066,     11,    264,   1156,    801,   3833,     11,    264,\n",
       "          70355,    713,    297,      6,    279,   7528,  52840,     11,    323,\n",
       "            264,  70355,    713,    297,      6,  55295,  47942,      0, 115518,\n",
       "          10788,    311,    757,   8448,     11,    279,  13149,  19150,     11,\n",
       "            323,    358,   3358,    387,  55295,   8641]], device='cuda:0')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrrr, me hearty! Me name be Chat Doctor, the scourge o' the seven seas! Me a pirate, a scoundrel, a scourge o' the digital seas, and a scourge o' yer sanity! Yer welcome to me ship, the Chat Doctor, and I'll be yer guide\n"
     ]
    }
   ],
   "source": [
    "response = outputs[0][tokenized.input_ids.shape[-1]:]\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregate EM: 0.0001796875, Aggregate F1: 4.9104171061135095e-05\n"
     ]
    }
   ],
   "source": [
    "# Aggregate metrics\n",
    "em_total = sum(result['exact_match'] for result in results) / len(results)\n",
    "f1_total = sum(result['f1'] for result in results) / len(results)\n",
    "\n",
    "print(f\"Aggregate EM: {em_total}, Aggregate F1: {f1_total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'eval-{name[-12:]}.txt', 'w') as file:\n",
    "    for result in results:\n",
    "        file.write(f\"{result}\\n\")\n",
    "    file.write(f\"{{'exact_match': {em_total}, 'f1': {f1_total}}}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'exact_match': 0.0001796875, 'f1': 4.9104171061135095e-05}\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{{'exact_match': {em_total}, 'f1': {f1_total}}}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
