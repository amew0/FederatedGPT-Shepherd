{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ft-medical.py for evaluation\n",
    "logg = lambda x: print(f\"------------------------ {x} ---------------------------\")\n",
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "import huggingface_hub\n",
    "import wandb\n",
    "wandb.require('core')\n",
    "from datetime import datetime\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from time import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/kunet.ae/ku5001069/.cache/huggingface/token\n",
      "Login successful\n",
      "------------------------ 240628145558 ---------------------------\n",
      "------------------------ llm-as-a-judge.py ---------------------------\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "load_dotenv()\n",
    "HF_TOKEN_WRITE = os.getenv(\"HF_TOKEN_WRITE\")\n",
    "huggingface_hub.login(token=HF_TOKEN_WRITE)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Set model and tokenizer paths\n",
    "ME = \"/dpc/kunf0097/l3-8b\"\n",
    "RUN_ID = datetime.now().strftime(\"%y%m%d%H%M%S\")\n",
    "logg(RUN_ID)\n",
    "logg('llm-as-a-judge.py')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f01f2a6d01ff4647a7d5c182ea1b2ba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "eval_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    eval_name, \n",
    "    cache_dir=f\"{ME}/tokenizer\", \n",
    "    pad_token_id=0,\n",
    ")\n",
    "\n",
    "eval_model = AutoModelForCausalLM.from_pretrained(\n",
    "    eval_name,\n",
    "    cache_dir=f\"{ME}/model\", \n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34323691f95f4c2d80843ae817747059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "# model to be evaluated\n",
    "name = \"amew0/l3-8b-medical-v240623023136\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    name, \n",
    "    cache_dir=f\"{ME}/tokenizer\", \n",
    "    pad_token_id=0,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    name,\n",
    "    cache_dir=f\"{ME}/model\", \n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenize prompt function\n",
    "def tokenize(prompt, tokenizer, add_eos_token=True):\n",
    "    tokenized = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return tokenized\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    # for eval, I think, the output part should not be there\n",
    "    prompt = \"\"\"<|start_header_id|>system<|end_header_id|> {}<|eot_id|><|start_header_id|>user<|end_header_id|> {}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "    prompt = prompt.format(data_point[\"instruction\"], data_point[\"input\"])\n",
    "    tokenized_full_prompt = tokenize(prompt, tokenizer=tokenizer)\n",
    "    return tokenized_full_prompt\n",
    "\n",
    "# Load and process dataset\n",
    "data_path = './data/1/eval_medical_2k.json'\n",
    "output_dir = f'{ME}/output/output_{RUN_ID}'\n",
    "data = load_dataset(\"json\", data_files=data_path)\n",
    "eval_dataset = data[\"train\"].map(generate_and_tokenize_prompt) # not shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_prompt_tokenizer(generated:str, output:str):\n",
    "    # we can add the question for context if we fill like\n",
    "    # [Later] structure (i.e how sentence-structurally the generated output resembled the expected one) and  \n",
    "    prompt = \"\"\"<|start_header_id|>system<|end_header_id|>\n",
    "You are going to act as an LLM evaluator to rate the answer of the medical chatbot on factualness (i.e how contextually the generated output followed the expected reply). Penalize it appropriately for any hallucination, lost of context, or trailing repetition. YOUR RESPONSE IS NOTHING ELSE BUT A FLOAT FROM 1.0 - 5.0. Where 1.0 indicates the context of the generated response is far from the expected one. And 5.0 represents otherwise. AGAIN IF YOUR GENERATED ANYTHING ELSE BUT A FLOAT YOU'RE GOING TO CRUSH MY SYSTEM!!<|eot_id|><|start_header_id|>user<|end_header_id|> \n",
    "### Expected: {}\n",
    "### Generated: {}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "    prompt = prompt.format(generated, output)\n",
    "    tokenized_full_prompt = tokenize(prompt, tokenizer=eval_tokenizer)\n",
    "    return tokenized_full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_score(text):\n",
    "    match = re.search(r'[-+]?\\d*\\.\\d+|\\d+', text)\n",
    "    return float(match.group(0)) if match else -1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_result = f'./out/results_{name[-12:]}.json'\n",
    "with open(json_result, 'w') as f:\n",
    "    f.write('[') \n",
    "def log2json(res):\n",
    "    with open(json_result, 'a') as f:\n",
    "        json.dump(res, f, ensure_ascii=False)\n",
    "        f.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i, example in enumerate(eval_dataset):\n",
    "    res = None\n",
    "    example[\"input_ids\"] = torch.LongTensor(example[\"input_ids\"]).to(model.device)\n",
    "    example[\"attention_mask\"] = torch.LongTensor(example[\"attention_mask\"]).to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=example[\"input_ids\"],\n",
    "        attention_mask = example[\"attention_mask\"],\n",
    "        max_new_tokens=512,\n",
    "        eos_token_id=[\n",
    "            tokenizer.eos_token_id,\n",
    "            tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "        ],\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "\n",
    "    response_ids = outputs[0][example[\"input_ids\"].shape[-1]:]\n",
    "    response = tokenizer.decode(response_ids, skip_special_tokens=True)\n",
    "    gt_response = example[\"output\"] # groundtruth\n",
    "\n",
    "    eval_tokenized = eval_prompt_tokenizer(response, gt_response)\n",
    "\n",
    "    evals_per_example = 2\n",
    "\n",
    "    llm_scores = []\n",
    "    for i in range(evals_per_example):\n",
    "        # use the evaluator here\n",
    "        eval_output = eval_model.generate(\n",
    "            input_ids=torch.LongTensor(eval_tokenized[\"input_ids\"]).to(model.device),\n",
    "            attention_mask = torch.LongTensor(eval_tokenized[\"attention_mask\"]).to(model.device),\n",
    "            max_new_tokens=128,\n",
    "            eos_token_id=[\n",
    "                tokenizer.eos_token_id,\n",
    "                tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "            ],\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "\n",
    "        eval_response = eval_output[0][eval_tokenized[\"input_ids\"].shape[-1]:]\n",
    "        llm_score = eval_tokenizer.decode(eval_response, skip_special_tokens=True)\n",
    "        llm_scores.append(extract_score(llm_score))\n",
    "\n",
    "\n",
    "    res = {\n",
    "        \"output\": gt_response,\n",
    "        \"generated\": response,\n",
    "        \"llm_scores\": llm_scores,\n",
    "        \"avg_llm_score\": sum(llm_scores)/len(llm_scores)\n",
    "    }\n",
    "    log2json(res)\n",
    "    \n",
    "    results.append(res)\n",
    "    del example\n",
    "    gc.collect()\n",
    "    gc.collect()\n",
    "    # if i == 0: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(json_result, 'w') as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
