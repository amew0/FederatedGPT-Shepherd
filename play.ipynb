{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "import fire\n",
    "import torch\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, AutoModelForCausalLM,  BitsAndBytesConfig\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "from fed_utils import FedAvg, client_selection, global_evaluation, GeneralClient\n",
    "import datasets\n",
    "from utils.prompter import Prompter\n",
    "datasets.utils.logging.set_verbosity_error()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode some text\n",
    "text = \"What are all the positions in hockey?\"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "# Generate predictions\n",
    "output = model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
    "# Decode the generated tokens to get the text\n",
    "predicted_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(predicted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and model\n",
    "global_model = \"chavinlo/alpaca-native\"\n",
    "tokenizer = LlamaTokenizer.from_pretrained(global_model)\n",
    "tokenizer.cache_dir = \"/dpc/kunf0007/amine/tokenizer\"\n",
    "tokenizer.pad_token_id = (0)\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization configuration\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    global_model,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0},\n",
    "    cache_dir=\"/dpc/kunf0007/amine/model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules= [\"q_proj\",\"k_proj\",\"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompter = Prompter('alpaca', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_len = 512\n",
    "def tokenize(prompt, add_eos_token=True):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=cutoff_len,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    if (\n",
    "            result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "            and len(result[\"input_ids\"]) < cutoff_len\n",
    "            and add_eos_token\n",
    "    ):\n",
    "        result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "        result[\"attention_mask\"].append(1)\n",
    "\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = prompter.generate_prompt(\n",
    "        data_point[\"instruction\"],\n",
    "        data_point[\"context\"],\n",
    "        data_point[\"response\"],\n",
    "    )\n",
    "    tokenized_full_prompt = tokenize(full_prompt)\n",
    "    return tokenized_full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "local_data_path = './data/1/local_training_0.json'\n",
    "local_output_dir = '/dpc/kunf0007/amine/output/local_output_0'\n",
    "local_data = load_dataset(\"json\", data_files=local_data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_train_dataset = local_data[\"train\"].shuffle().map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from trl import SFTTrainer\n",
    "\n",
    "gradient_accumulation_steps = 8 // 4\n",
    "def build_local_trainer(\n",
    "    tokenizer=tokenizer,\n",
    "    local_micro_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    local_learning_rate=3e-4,\n",
    "    group_by_length=False,\n",
    "):\n",
    "    train_args = transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=local_micro_batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        warmup_steps=1,\n",
    "        num_train_epochs=1,\n",
    "        learning_rate=local_learning_rate,\n",
    "        fp16=False,\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_torch\",\n",
    "        output_dir=local_output_dir,\n",
    "        group_by_length=group_by_length,\n",
    "        dataloader_drop_last=False,\n",
    "    )\n",
    "    local_trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=local_train_dataset,\n",
    "        args=train_args,\n",
    "        tokenizer=tokenizer,\n",
    "        dataset_text_field=\"instruction\",\n",
    "        max_seq_length=512,\n",
    "    )\n",
    "    return local_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_trainer = build_local_trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import copy\n",
    "def initiate_local_training():\n",
    "    model.config.use_cache = False\n",
    "    params_dict_old = copy.deepcopy(\n",
    "        OrderedDict(\n",
    "            (name, param.detach())\n",
    "            for name, param in model.named_parameters()\n",
    "            if \"default\" in name\n",
    "        )\n",
    "    )\n",
    "    params_dict_new = OrderedDict(\n",
    "        (name, param.detach())\n",
    "        for name, param in model.named_parameters()\n",
    "        if \"default\" in name\n",
    "    )\n",
    "    model.state_dict = (\n",
    "        lambda instance, *_, **__: get_peft_model_state_dict(\n",
    "            instance, params_dict_new, \"default\"\n",
    "        )\n",
    "    ).__get__(model, type(model))\n",
    "initiate_local_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def train():\n",
    "\n",
    "    gc.collect()\n",
    "    gc.collect()\n",
    "    local_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"/dpc/kunf0007/amine/model/mylora-shepherd-v0\")\n",
    "tokenizer.save_pretrained(\"/dpc/kunf0007/amine/model/mylora-shepherd-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!transformers-cli repo create mylora-shepherd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(\"mylora-shepherd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_ = LlamaTokenizer.from_pretrained(local_path, cache_dir=\"/dpc/kunf0007/amine\")\n",
    "model_ = AutoModelForCausalLM.from_pretrained(\n",
    "    local_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config,\n",
    "    low_cpu_mem_usage=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode some text\n",
    "text_ = \"What are all the positions in hockey?\"\n",
    "input_ids_ = tokenizer_.encode(text_, return_tensors=\"pt\")\n",
    "\n",
    "# Generate predictions\n",
    "output_ = model_.generate(input_ids_, max_length=50, num_return_sequences=1)\n",
    "\n",
    "# Decode the generated tokens to get the text\n",
    "predicted_text_ = tokenizer_.decode(output_[0], skip_special_tokens=True)\n",
    "print(predicted_text_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(tokenizer, model, text):\n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=50,\n",
    "        num_return_sequences=1,\n",
    "        num_beams=5,  # Using beam search with 5 beams\n",
    "        temperature=1.0,  # Default temperature\n",
    "        top_k=50,  # Top-K sampling\n",
    "        top_p=0.95,  # Nucleus sampling\n",
    "        do_sample=True  # Enable stochastic sampling\n",
    "    )\n",
    "\n",
    "    # Decode the generated tokens to get the text\n",
    "    predicted_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return predicted_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode( \"give me a summary of what a kidney donor chain is.\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=  \"give me a summary of what a kidney donor chain is.\"\n",
    "infer(tokenizer, model, text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
